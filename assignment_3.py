# -*- coding: utf-8 -*-
"""Assignment 3

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Gl6o_1CwTFiMWOg7H44jSdvlVx70CORw

#Data Prep
"""

from sklearn.linear_model import LinearRegression
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.preprocessing import MinMaxScaler
from itertools import combinations
from scipy.stats import f
import matplotlib.pyplot as plt
import seaborn as sns

watermelon_url = 'https://raw.githubusercontent.com/kellerdevin/CPSC-370-Assignment-3-Open-ET/main/Watermelon.csv?token=GHSAT0AAAAAACIFEBFVNVMWBKAQ5WMHDIHUZIY2AMQ'
watermelon_df = pd.read_csv(watermelon_url)

corn_url = 'https://raw.githubusercontent.com/kellerdevin/CPSC-370-Assignment-3-Open-ET/main/Corn.csv?token=GHSAT0AAAAAACIFEBFU5THTHS7VF6GXDJJSZIY2K4Q'
corn_df = pd.read_csv(corn_url)

peaches_url = 'https://raw.githubusercontent.com/kellerdevin/CPSC-370-Assignment-3-Open-ET/main/Peaches.csv'
peaches_df = pd.read_csv(peach_url)

fallow_url = 'https://raw.githubusercontent.com/kellerdevin/CPSC-370-Assignment-3-Open-ET/main/Fallow.csv'
fallow_df = pd.read_csv(fallow_url)

corn_df.head()

# Calculate the variances for the "Ensemble ET" variable for each sample
var_corn = corn_df['Ensemble ET'].var()
var_peaches = peaches_df['Ensemble ET'].var()
var_watermelon = watermelon_df['Ensemble ET'].var()
var_fallow = fallow_df['Ensemble ET'].var()

print("Corn var: " + str(var_corn.round(2)))
print("Peaches var: " + str(var_peaches.round(2)))
print("Watermellon var: " + str(var_watermelon.round(2)))
print("Fallow var: " + str(var_fallow.round(2)))

"""F test"""

# List of sample names and their variances
samples = [('Corn', var_corn), ('Fallow', var_fallow), ('Peaches', var_peaches), ('Watermelon', var_watermelon)]

# Calculate F-values for each pair of samples
f_values = {}
p_values = {}

for (sample1_name, var1), (sample2_name, var2) in combinations(samples, 2):
    f_value = max(var1, var2) / min(var1, var2)
    pair_name = f"{sample1_name} vs {sample2_name}"
    f_values[pair_name] = f_value

    # Calculate degrees of freedom for each sample
    df1 = len(eval(f"{sample1_name.lower()}_df['Ensemble ET'].dropna()")) - 1
    df2 = len(eval(f"{sample2_name.lower()}_df['Ensemble ET'].dropna()")) - 1

    # Calculate the p-value
    p_value = 1 - f.cdf(f_value, df1, df2)
    p_values[pair_name] = p_value

print("F-values:", f_values)
print("p-values:", p_values)

# Check for significant differences in variances
for pair, p_value in p_values.items():
    if p_value < 0.05:
        print(f"The variances between {pair} are significantly different (p-value: {p_value:.4f}).")
    else:
        print(f"The variances between {pair} are not significantly different (p-value: {p_value:.4f}).")

# Drop missing values in 'Ensemble ET' for all DataFrames
corn_et = corn_df['Ensemble ET'].dropna()
peaches_et = peaches_df['Ensemble ET'].dropna()
watermelon_et = watermelon_df['Ensemble ET'].dropna()
fallow_et = fallow_df['Ensemble ET'].dropna()

# Combine the four data sets into one for easy plotting
combined_df = pd.DataFrame({
    'Corn': corn_et,
    'Fallow': fallow_et,
    'Peaches': peaches_et,
    'Watermelon': watermelon_et
})

combined_df

# Melt the DataFrame to long format for plotting
melted_df = pd.melt(combined_df.reset_index(), id_vars=['index'], value_vars=['Corn', 'Fallow', 'Peaches', 'Watermelon'])
melted_df.columns = ['Index', 'Crop', 'Ensemble ET']

melted_df

# Create the boxplot
plt.figure(figsize=(10, 6))
sns.boxplot(x='Crop', y='Ensemble ET', data=melted_df)
plt.title('Variation in Ensemble ET Across Different Crops')
plt.xlabel('Crop Type')
plt.ylabel('Ensemble ET')
plt.show()

"""## Linear Regression"""

from sklearn.linear_model import LinearRegression
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error, r2_score

"""LR Function for a given crop"""

# Function to perform Linear Regression and plot for a given crop dataset
def perform_linear_regression_and_plot(ax, df, crop_name):
    # Prepare the data
    data = df[['NDVI', 'Precip (gridMET)', 'Ensemble ET']].dropna()

    # Features (P and ET) and Target (NDVI)
    X = data[['Precip (gridMET)', 'Ensemble ET']]
    y = data['NDVI']

    # Split the data into training and testing sets
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

    # Initialize the model
    lr_model = LinearRegression()

    # Fit the model to the training data
    lr_model.fit(X_train, y_train)

    # Make predictions on the test set
    y_pred = lr_model.predict(X_test)

    # Scatter plot of the actual vs predicted NDVI
    ax.scatter(y_test, y_pred, color='blue')
    ax.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'k--', lw=2)
    ax.set_xlabel('Actual NDVI')
    ax.set_ylabel('Predicted NDVI')
    ax.set_title(f'{crop_name}')

# Create a 2x2 grid for the four crops
fig, axs = plt.subplots(2, 2, figsize=(16, 12))
fig.suptitle('Actual vs Predicted NDVI for Different Crops')

# Perform Linear Regression and plot for each crop
perform_linear_regression_and_plot(axs[0, 0], corn_df, 'Corn')
perform_linear_regression_and_plot(axs[0, 1], fallow_df, 'Fallow')
perform_linear_regression_and_plot(axs[1, 0], peaches_df, 'Peaches')
perform_linear_regression_and_plot(axs[1, 1], watermelon_df, 'Watermelon')

plt.tight_layout(rect=[0, 0.03, 1, 0.95])
plt.show()